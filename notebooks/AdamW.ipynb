{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598235973612",
   "display_name": "Python 3.8.3 64-bit ('venv3d': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'name': 'Adam',\n 'learning_rate': 0.001,\n 'decay': 0.0,\n 'beta_1': 0.9,\n 'beta_2': 0.999,\n 'epsilon': 1e-07,\n 'amsgrad': False}"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "adam.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on method apply_gradients in module tensorflow.python.keras.optimizer_v2.optimizer_v2:\n\napply_gradients(grads_and_vars, name=None, experimental_aggregate_gradients=True) method of tensorflow.python.keras.optimizer_v2.adam.Adam instance\n    Apply gradients to variables.\n    \n    This is the second part of `minimize()`. It returns an `Operation` that\n    applies gradients.\n    \n    The method sums gradients from all replicas in the presence of\n    `tf.distribute.Strategy` by default. You can aggregate gradients yourself by\n    passing `experimental_aggregate_gradients=False`.\n    \n    Example:\n    \n    ```python\n    grads = tape.gradient(loss, vars)\n    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n    # Processing aggregated gradients.\n    optimizer.apply_gradients(zip(grads, vars),\n        experimental_aggregate_gradients=False)\n    \n    ```\n    \n    Args:\n      grads_and_vars: List of (gradient, variable) pairs.\n      name: Optional name for the returned operation. Default to the name passed\n        to the `Optimizer` constructor.\n      experimental_aggregate_gradients: Whether to sum gradients from different\n        replicas in the presense of `tf.distribute.Strategy`. If False, it's\n        user responsibility to aggregate the gradients. Default to True.\n    \n    Returns:\n      An `Operation` that applies the specified gradients. The `iterations`\n      will be automatically increased by 1.\n    \n    Raises:\n      TypeError: If `grads_and_vars` is malformed.\n      ValueError: If none of the variables have gradients.\n\n"
    }
   ],
   "source": [
    "help(adam.apply_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on method minimize in module tensorflow.python.keras.optimizer_v2.optimizer_v2:\n\nminimize(loss, var_list, grad_loss=None, name=None) method of tensorflow.python.keras.optimizer_v2.adam.Adam instance\n    Minimize `loss` by updating `var_list`.\n    \n    This method simply computes gradient using `tf.GradientTape` and calls\n    `apply_gradients()`. If you want to process the gradient before applying\n    then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n    of using this function.\n    \n    Args:\n      loss: A callable taking no arguments which returns the value to minimize.\n      var_list: list or tuple of `Variable` objects to update to minimize\n        `loss`, or a callable returning the list or tuple of `Variable` objects.\n        Use callable when the variable list would otherwise be incomplete before\n        `minimize` since the variables are created at the first time `loss` is\n        called.\n      grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n      name: Optional name for the returned operation.\n    \n    Returns:\n      An `Operation` that updates the variables in `var_list`. The `iterations`\n      will be automatically increased by 1.\n    \n    Raises:\n      ValueError: If some of the variables are not `Variable` objects.\n\n"
    }
   ],
   "source": [
    "help(adam.minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}